# Assumptions for Pearson

Pearson's correlation is a type of bivariate correlation that measures the relationship between two variables. Testing the significance of the correlation coefficient requires the following assumptions:

-   Both variables are measured on an interval scale
-   Both variables are normally distributed
-   The relationship between the variables is linear

If these assumptions are violated, we can use alternative correlations - such as Spearman's or Kendall's - or use bootstrapping.

# Real-World Pearson Correlation: Step-by-Step

Let's look at an example from Andy Field's textbook.

A psychologist was interested in how exam stress and revision time affect exam performance. She created and validated a questionnaire, the Exam Anxiety Questionnaire (EAQ), which measured state anxiety related to exams on a scale from 0 to 100. Anxiety was measured before the exam, exam performance was recorded as the percentage score achieved, and the number of hours spent revising was also measured.

```{r, include=FALSE}
# Packages used
library(ggplot2)
library(dplyr)
library(broom)
```

## 1. Data Entry

Data entry for correlation is straightforward because each variable is entered in a separate column.

```{r}
url <- "https://studysites.sagepub.com/dsur/study/DSUR%20Data%20Files/Chapter%206/Exam%20Anxiety.dat"
exam_data <- read.table(url, header = TRUE)
head(exam_data)
```

## 2. Scatter Plots

After getting the data into the right format, we would typically examine the assumptions for Pearson’s correlation in detail. Since I have already explored the assumptions for various tests in *Statistical Assumptions and Data Screening*, let’s move straight to the scatter plots.

We can visualize the associations between: anxiety score & exam performance, revision hours & exam performance, and revision hours & anxiety score. However, to avoid making this document too long, we will only focus on the association between anxiety score & exam performance.

Our data are split by `Gender`, allowing us to obtain two types of results: (1) the relationship between anxiety and performance **within each gender**, and (2) the relationship between anxiety and performance while controlling for gender. Since we are focusing on Pearson's correlation, we will skip option 2 as it requires a partial correlation.

```{r, collapse=TRUE}
# Anxiety and Performance Within Gender
ggplot(exam_data, aes(x = Anxiety, y = Exam)) + 
  geom_point(aes(color = Gender), alpha = 0.5) + 
  geom_smooth(aes(color = Gender, fill = Gender), method="lm", alpha = 0.2) + 
  scale_color_manual(values = c("black", "blue")) +
  scale_fill_manual(values = c("black", "blue")) +
  scale_x_continuous(breaks = c(20, 40, 60, 80)) +
  scale_y_continuous(breaks = c(20, 40, 60, 80, 100, 120)) +
  labs(x="Exam Anxiety", y="Exam Performance %") 
```

The regression lines show that for both males and females, higher exam anxiety is associated with lower exam performance. This indicates an inverse relationship: as anxiety increases, performance decreases, and vice versa. However, the plot only subjectively reveals the strength of this association — the Pearson’s correlation, which we will calculate later, quantifies both its strength and direction.

Comparing males and females, the relationship appears stronger for males, as shown by a steeper slope. Males tend to perform better than females at low anxiety levels, while females outperform males when anxiety is high. Nonetheless, the plot alone does not indicate whether these differences are statistically significant — to test whether the slope of Performance vs Anxiety differs by Gender we would use Fisher's Z-transformations.

Moreover, when we consider the Confidence Interval (CI), represented by the shaded area around each line, we see that for females (wider CI) the estimation is less precise compared to males. Precision tells us how reliably the effect (e.g., the slope) is estimated — how much uncertainty there is around that estimate.

## 3. Pearson's Correlation

For testing the correlation between two variables, R provides the `cor.test()` function which takes this general form:

-   `x` is a numeric variable

-   `y` is another numeric variable

-   `alternative` specifies whether you want to do a two-tailed test (`alternative = "two.sided"`), which is the default, or whether you predict that the correlation will be less than zero (i.e., negative) or more than zero (i.e., positive), in which case you can use `alternative = "less"` and `alternative = "greater"`, respectively. Note that **the hypothesis should be decided before you see the data** (stop p-hacking).

-   `method` enables you to specify whether you want "pearson" or "spearman" correlations.

-   `conf.level` allows you to specify the width of the confidence interval computer for the correlation. Default is 0.95 (`conf.level = 0.95`).

**Note** that `cor.test()` returns `NA` if either variable contains missing values, so any NAs must be handled before running the test.

Let’s apply this function to our data to examine the correlation between anxiety and exam performance for males and females. I will omit the `alternative` and `conf.level` parameters since the defaults suffice for our purposes.

```{r}
exam_data %>%
  group_by(Gender) %>%
  summarise(tidy(cor.test(Anxiety, Exam, method = "pearson")), .groups = "drop")
```

The output displays the following for each group:

-   `estimate`: The observed Pearson correlation coefficient ($r$). Values around ±0.1 indicate a small effect, ±0.3 a medium effect, and ±0.5 or greater a large effect.

-   `statistic`: The t-value which measures how far the observed correlation is from zero relative to its standard error. A large absolute $t$ indicates strong evidence against the null hypothesis, while a small absolute `t` indicates weak evidence. Nevertheless, t-values do not have a universal "effect size" threshold.

-   `p.value`: Probability of observing the correlation if there’s actually no relationship. If high, weak evidence for a correlation; if low, strong evidence for a correlation.

-   `parameter`: Degrees of freedom used for the t-test (sample size minus 2).

-   `conf.low` and `conf.high`: Showing the range where the true correlation likely falls. A wide range indicates uncertainty about the true correlation, while a narrow range suggests a precise estimate. If the range includes zero, the true correlation may be zero.

-   `method`: The correlation method used.

-   `alternative`: Alternative hypothesis tested.

The results of the correlation analysis show the relationship between anxiety and exam performance within each gender. For females, there was a moderate negative correlation, while for males, the negative correlation was even stronger. The confidence interval for both groups does not cross zero, which suggests that the population correlation is negative. This gives us good reason to believe that exam anxiety and exam performance are, in reality, negatively related.

## 4. Comparing Correlations

Methods for comparing correlations differ between independent and dependent samples because independence changes the sampling distribution. The sampling distribution gives us the standard error, which we then use to compute a $Z$ (or $t$) statistic. That statistic tells us how unusual our observed difference is under the null hypothesis.

Comparing correlation coefficients from **two independent samples**, such as males and females, can be done by transforming the coefficients using Fisher's Z-transformation. This gives a Z-test statistic, representing how many standard errors apart the two Fisher Z-transformed correlations are. Once we obtain the $Z$ statistic, we can assess whether the difference between the male and female correlations is statistically significant.

```{r, collapse=TRUE}
females_r <- -0.381 # correlation coeff
females_n <- nrow(subset(exam_data, Gender == "Female")) # sample size 

males_r <- -0.506
males_n <- nrow(subset(exam_data, Gender == "Male")) 

# Fisher Z-transformation
females_z <- 0.5 * log((1 + females_r) / (1 - females_r))
males_z <- 0.5 * log((1 + males_r) / (1 - males_r))

# Standard error 
se <- sqrt(1 / (females_n - 3) + 1 / (males_n - 3))

# Z-test statistic
z <- (females_z - males_z) / se

# p-value (two-tailed)
p <- 2 * (1 - pnorm(abs(z)))

cat("Z-test statistic: ", z, "\n")
cat("p-value: ", p)
```

A small absolute value for the $Z$ statistic implies a small difference between correlations. As shown by the p-value, the correlation between exam anxiety and performance is not significantly different in men and women.

## 5. Interpreting Effect Sizes with $R^2$

We have seen that exam anxiety and exam performance are inversely associated, but we cannot make direct conclusions about causality from a correlation. We can, however, take the correlation coefficient a step further. The coefficient of determination, indicated as $R^2$, represents the squared correlation coefficient which measures the proportion of variance that is **shared** between two variables. 

We are interested in how much of the variability in exam performance is shared, or accounted for, by exam anxiety. Taking males as an example, we know that $r = -0.506$. If we square this to get the coefficient of determination we would get $R^2 = 0.256$, and if we then convert this value into a percentage (multiply by 100) we can say that for males, exam anxiety shares 25.6% of the variability in exam performance. This leaves 74.4% of the variability still to be accounted for by other variables.

$R^2$ is a very useful but often misunderstood statistic. People sometimes take it to imply causality when reading statements like 'x% of the variance in variable A is explained by variable B.' However, $R^2$ cannot be used to infer causal relationships, because it only reflects how strongly variables are associated, not whether one causes the other. In any correlation, causality between variables cannot be assumed because there may be other measured or unmeasured variables affecting the results. Correlation coefficients - and by extension coefficients of determination - give no indication of the direction of causality, meaning that even when variables are correlated, we can never truly know if one affects the other. Exam anxiety might share some of the variation in exam performance, but it does not necessarily cause this variation.




