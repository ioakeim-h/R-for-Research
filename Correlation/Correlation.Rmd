The relationship between variables can be statistically expressed by looking at two measures: *covariance* and the *correlation coefficient*.

# Covariance

The simplest way to look at whether two variables are associated is to look at whether they *covary*. A positive covariance indicates that as one variable deviates from the mean, the other variable deviates in the same direction. On the other hand, a negative covariance indicates that as one variable deviates from the mean (e.g. increases), the other deviates from the mean in the opposite direction (e.g. decreases).

The problem with covariance is that it is not a standardized measure as it is scale-dependent: its value changes if you change the units of either variable, making it hard to compare covariances in an objective way across variable pairs or datasets.

# Correlation Coefficient

The correlation coefficient is the standardized form of covariance and provides a more interpretable, unitless measure of association between variables. It is calculated by dividing the covariance by the product of the variables’ standard deviations, which scales the value to a fixed range between –1 and 1. A correlation coefficient of 1 indicates a perfect positive linear relationship, –1 a perfect negative, and 0 no linear relationship.

Generally, values around ±0.1 indicate a small effect (weak relationship), ±0.3 a medium effect (moderate relationship), and ±0.5 or greater a large effect (strong relationship). *However, these thresholds are approximate guidelines and should not replace interpretation within the specific research context and literature*.

# Testing Correlation Coefficients

Although we can directly interpret the effect size of a correlation coefficient, we can also test the hypothesis that the correlation is different from zero (i.e., different from “no relationship”). Significance shows whether an effect likely isn’t due to chance, while effect size shows how large or meaningful that effect is.

## Bivariate Correlations

For testing the correlation between two variables, R provides the `cor.test()` function which takes two variables and a method. Main methods include:

-   "pearson" - use Pearson correlation when both variables are continuous, normally distributed, and have a linear relationship with each other

-   "spearman" - use Spearman correlation when variables are ordinal, not normally distributed, or have a monotonic but non-linear relationship.

-   "kendall" - use Kendall’s tau when you want a robust, rank-based correlation measure for small samples or data with many tied ranks, especially for ordinal or non-normal data.

The output of the `cor.test()` function includes:

-   **t**: The t-statistic for testing if the correlation differs from zero. Measures how far the observed correlation is from zero relative to its standard error.

-   **df**: Degrees of freedom (sample size minus 2).

-   **p-value**: Probability of observing the correlation if there’s actually no relationship. If high, weak evidence for a correlation; if low, strong evidence for a correlation.

-   **alternative hypothesis**: Testing if the true correlation is not zero (two-tailed test).

-   **95% confidence interval**: Showing the range where the true correlation likely falls.

-   **sample estimate (cor)**: The observed Pearson correlation coefficient.

### Interpretation of Results

```{r}
# Example data with weak/no correlation
x <- c(1, 2, 3, 4, 5)
y <- c(5, 7, 4, 6, 5)

cor.test(x, y, method = "pearson")
```

-   **t = -0.24**: Very close to zero, indicating a weak effect.
-   **p-value = 0.82**: Very high, meaning we fail to reject the null hypothesis; there’s no evidence of a correlation different from zero.
-   **95% CI (-0.91 to 0.85)**: Wide interval including zero, indicating uncertainty about the true correlation and that it could be zero.
-   **cor = -0.14**: Small negative correlation, but not statistically significant.

```{r}
# Example data with strong positive correlation
x <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
y <- c(2.1, 2.5, 3.7, 4.1, 5.3, 5.9, 7.2, 7.8, 8.5, 9.6)

cor.test(x, y, method = "pearson")  
```

-   **t = 36.72**: Very large test statistic, showing the observed correlation is far from zero relative to its standard error.
-   **p-value = 0.0000000003317**: Extremely (and unrealistically) low, showing strong evidence for rejecting the null hypothesis; there is a correlation.
-   **95% CI (0.987 to 0.999)**: Narrow interval very close to 1, indicating a very precise estimate of a strong positive correlation.
-   **cor = 0.99**: Indicates a near-perfect strong positive linear relationship.

## Partial Correlations

Partial correlation: correlation between two variables while controlling for others

...

## Interpreting Confidence Intervals

Confidence intervals (CIs) show the range within which the true parameter (e.g., correlation) is likely to fall, with a given level of confidence (usually 95%).

For example, a 95% CI means: if we repeated the study many times, 95% of those intervals would contain the true value.

If the CI includes zero (for correlation), it suggests the effect might be zero—so the result isn’t statistically significant. If it doesn’t include zero, the effect is likely real.

The lower and upper bounds of the confidence interval represent the range where the true correlation coefficient is likely to fall with a specified confidence level (usually 95%).

-   **Lower bound:** The smallest value the true correlation is expected to be.
-   **Upper bound:** The largest value the true correlation is expected to be.

A narrow confidence interval means the analysis is precise and reliable, while a wide interval - usually due to small sample size or high data variability - indicates greater uncertainty and less reliability in the results.

# Reporting Results

The Pearson's correlation coefficient is denoted both by *r* and *R*. Typically, the upper-case form is used in the context of regression because it represents the multiple correlation coefficient; however, when we square *r* an upper case *R* is used.

## WARNING: Association does not imply causation

Correlation coefficients give no indication of the direction of causality. Additionally, in any correlation, causality between variables cannot be assumed because there may be other measured or unmeasured variables affecting the results.
